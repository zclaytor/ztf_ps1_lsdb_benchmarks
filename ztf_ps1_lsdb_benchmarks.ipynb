{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39a5f41a-fb9b-422f-a242-77c3a51fabf2",
   "metadata": {},
   "source": [
    "# Cross-Matching ZTF and Pan-STARRS using LSDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18569be-ef81-41ee-b5d2-4c21af0e96a7",
   "metadata": {},
   "source": [
    "[LSDB](https://lsdb.io) is a useful package for performing large cross-matches between source catalogs. It's built to run across multiple nodes with Dask parallelization, but even without parallelization it is high-performance. Here we will benchmark the performance of LSDB on the NASA Fornax platform with and without Dask.\n",
    "\n",
    "We will start small, trying to cross-match 10,000 sources from ZTF with Pan-STARRS. We will then scale up by factors of roughly 10 until either (a) the platform can no longer handle the load, or (b) we do the full cross-match.\n",
    "\n",
    "For each level, we want to know the performance with (1) no Dask, (2) minimal Dask - like 2 workers, (3) bigger Dask - as many workers as we can use, and (4) auto-scaling Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949527e8-8908-464e-94a1-aa148f90128c",
   "metadata": {},
   "source": [
    "## Preconfiguring the Run\n",
    "First choose the number of rows we want to cross-match and our `dask` environment. Note that you can also configure `dask` using the `daskhub` options on Fornax. If you go this route, leave `dask_workers = 0` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079346f-926a-484d-8bdf-737fe5f52b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The left table will have about this many rows. The cross-matched product will have fewer.\n",
    "Nrows = 10_000\n",
    "\n",
    "# dask_workers can be 0 (no dask), 1-Ncores, or \"scale\" for auto-scaling\n",
    "dask_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef23d6ee-ccf2-4881-8bd7-e60eecc719a6",
   "metadata": {},
   "source": [
    "## Imports\n",
    "We require the use of `astropy` for coordinates and units, and `lsdb` to read the catalogs and perform the cross-match. Optionally, we will set up `dask` parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74937f2-4d86-4b69-b7d0-4c32c31602ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "from lsdb.core.search import ConeSearch\n",
    "\n",
    "import lsdb\n",
    "\n",
    "# Set up dask cluster\n",
    "if dask_workers != 0:\n",
    "    from dask.distributed import Client, LocalCluster\n",
    "    cluster = LocalCluster()\n",
    "\n",
    "    if dask_workers == \"scale\":\n",
    "        import os\n",
    "        cluster.adapt(minimum_cores=1, maximum_cores=os.cpu_count())\n",
    "    else:\n",
    "        cluster.scale(dask_workers)\n",
    "        \n",
    "    client = Client(cluster)\n",
    "    client\n",
    "\n",
    "# Select the search radius to give us the right number of rows\n",
    "radius = { # Nrows: radius_arcseconds\n",
    "           10_000:     331,\n",
    "          100_000:    1047,\n",
    "        1_000_000:    3318,\n",
    "       10_000_000:  11_180,\n",
    "      100_000_000:  33_743,\n",
    "    1_000_000_000: 102_000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f51f26-8f8c-4c93-8f4a-15a3bcca57a1",
   "metadata": {},
   "source": [
    "## Read in catalogs and downselect ZTF to Nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b18cbe-9172-41f9-9b86-2614a4d8c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sky area. Here we're using the Kepler field.\n",
    "c = SkyCoord('19:22:40  +44:30:00', unit=(u.hourangle, u.deg))\n",
    "cone_ra, cone_dec = c.ra.value, c.dec.value\n",
    "radius_arcsec = radius[Nrows]\n",
    "cone_filter = ConeSearch(cone_ra, cone_dec, radius_arcsec)\n",
    "\n",
    "# Read ZTF DR20\n",
    "ztf_path = (\"s3://irsa-mast-tike-spitzer-data/data/ZTF/dr20/objects/hipscat/ztf-dr20-objects-hipscat\")\n",
    "ztf_piece = lsdb.read_hipscat(ztf_path, columns=[\"oid\", \"ra\", \"dec\"], search_filter=cone_filter)\n",
    "\n",
    "# Read Pan-STARRS DR2\n",
    "ps1_path = \"s3://stpubdata/panstarrs/ps1/public/hipscat/otmo\"\n",
    "ps1 = lsdb.read_hipscat(ps1_path, storage_options={'anon': True},\n",
    "    columns=[\"objName\",\"objID\",\"raMean\",\"decMean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb021fc-378c-4a5d-89bc-54acca2230ef",
   "metadata": {},
   "source": [
    "## Initialize the crossmatch and compute, measuring the time elapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51bd56e-8011-4011-bda1-60f04a2e6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the cross-match actually takes very little time\n",
    "ztf_x_ps1 = ztf_piece.crossmatch(ps1, radius_arcsec=1, n_neighbors=1, suffixes=(\"_ztf\", \"_ps1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ac550-0912-4d37-b4c9-2e5fe192ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Executing the cross-match does take time\n",
    "xmatch = ztf_x_ps1.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d8b6c-8e40-43d6-998e-6385a0767b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the resulting table\n",
    "print(f\"Number of rows in:  {len(ztf_piece.compute()):,d}\")\n",
    "print(f\"Number of rows out: {len(xmatch):,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72841776-7782-4481-a448-c8d5bca665c7",
   "metadata": {},
   "source": [
    "## Record benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d82f3-c255-422c-ae35-67eda05b15d3",
   "metadata": {},
   "source": [
    "Benchmarks on Fornax XLarge instance using \n",
    "- no dask (t0)\n",
    "- one dask worker (t1)\n",
    "- two dask workers (t2)\n",
    "- four (t4)\n",
    "- eight (t8)\n",
    "- sixteen (t16)\n",
    "- autoscaling dask with 1-128 cores (tX)\n",
    "  \n",
    "| Nrows |  Nout | t0 (s) | t1 (s) | t2 (s) | t4 (s) | t8 (s) | t16 (s) | tX (s) |\n",
    "| ----- | ----- | ------ | ------ | ------ | ------ | ------ | ------- | ------ |\n",
    "| 1e4   |  8593 |  1.46  |  6.37  |  5.80  |  5.88  |  5.53  |   5.59  |  5.43  |\n",
    "| 1e5   | 85237 |  2.12  |  6.50  |  5.99  |  6.18  |  5.62  |   5.71  |  5.79  |\n",
    "| 1e6   | 8.4e5 |  3.46  |  8.84  |  6.88  |  7.07  |  6.35  |   6.39  |  6.78  |\n",
    "| 1e7   | 8.4e6 |  27.8  |  33.0  |  21.0  |  15.0  |  11.8  |  11.4   |  11.4  |\n",
    "| 1e8   | 8.6e7 |  192   |    -   |    -   |    -   |    -   |    -    |    -   |\n",
    "\n",
    "(\"-\" indicates out-of-memory behavior.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e41a2-c482-494e-9e44-ecace7c67abc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Fornax is capable of hosting cross-matches between large catalogs. There is no performance enhancement with `dask` until cross-matching ~10 million sources, at which point you get roughly a factor of two improvement at best. Larger than that hits memory issues with `dask`, and takes hours without `dask` (although I haven't actually finished the 1e8 match). There are ways to configure the maximum memory used by a `dask` worker, which I haven't yet explored. That might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c9dd2-d298-45ad-ac0a-e0a0000be05c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
